#!/usr/bin/env python
"""
Test script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
"""
import sys
import os

# Add paths
MCP_DIR = os.path.join(os.path.dirname(__file__), 'mcp_dream_analysis')
sys.path.insert(0, MCP_DIR)

from models.expert_dream_interpreter import ExpertDreamInterpreter

def test_tokenization_fixes():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥"""
    
    expert = ExpertDreamInterpreter()
    
    # Test cases with tokenization problems
    test_cases = [
        {
            'input': "‡∏ù‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏á‡∏π‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å",  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á -> ‡∏¢‡πà‡∏≤‡∏á
            'expected_fixed': "‡∏ù‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏á‡∏π‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å",
            'should_find': ['‡∏á‡∏π']
        },
        {
            'input': "‡∏ù‡∏±‡∏ô‡∏°‡πà‡∏ß‡∏¢‡∏î‡∏µ‡πÄ‡∏´‡πá‡∏ô‡∏ä‡πâ‡∏≤‡∏á‡∏™‡∏µ‡∏ó‡∏≠‡∏á",  # ‡∏°‡∏µ -> ‡∏°‡πà‡∏ß‡∏¢  
            'expected_fixed': "‡∏ù‡∏±‡∏ô‡∏°‡∏µ‡∏î‡∏µ‡πÄ‡∏´‡πá‡∏ô‡∏ä‡πâ‡∏≤‡∏á‡∏™‡∏µ‡∏ó‡∏≠‡∏á",
            'should_find': ['‡∏ä‡πâ‡∏≤‡∏á', '‡∏ó‡∏≠‡∏á']
        },
        {
            'input': "‡∏ù‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏û‡∏£‡∏∞‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏á‡∏ö",
            'expected_fixed': "‡∏ù‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏û‡∏£‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏á‡∏ö", 
            'should_find': ['‡∏û‡∏£‡∏∞']
        },
        {
            'input': "‡πÄ‡∏î‡πá‡∏Å‡∏ó‡∏≤‡∏£‡∏Å‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å",
            'expected_fixed': "‡πÄ‡∏î‡πá‡∏Å‡∏ó‡∏≤‡∏£‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å",
            'should_find': ['‡πÄ‡∏î‡πá‡∏Å‡∏ó‡∏≤‡∏£‡∏Å']
        },
        {
            'input': "‡∏ô‡πâ‡∏≥‡πÉ‡∏™‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏ß‡∏¢",
            'expected_fixed': "‡∏ô‡πâ‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏ß‡∏¢",
            'should_find': ['‡∏ô‡πâ‡∏≥']
        }
    ]
    
    print("üîß ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢")
    print("=" * 60)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nüß™ Test Case {i}")
        print(f"Input (‡∏õ‡∏±‡∏ç‡∏´‡∏≤):     {test_case['input']}")
        
        # Test tokenization fix
        fixed_text = expert._fix_thai_tokenization(test_case['input'].lower())
        print(f"Fixed Text:        {fixed_text}")
        print(f"Expected:          {test_case['expected_fixed'].lower()}")
        
        # Check if fix worked correctly
        fix_correct = fixed_text == test_case['expected_fixed'].lower()
        print(f"Fix Status:        {'‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á' if fix_correct else '‚ùå ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î'}")
        
        # Test symbol detection
        result = expert.interpret_dream(test_case['input'])
        found_symbols = result.get('main_symbols', [])
        
        print(f"Symbols Found:     {found_symbols}")
        print(f"Should Find:       {test_case['should_find']}")
        
        # Check if expected symbols were found
        symbol_match = any(symbol in found_symbols for symbol in test_case['should_find'])
        print(f"Symbol Detection:  {'‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á' if symbol_match else '‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå'}")
        
        if result.get('predicted_numbers'):
            top_numbers = [pred['number'] for pred in result['predicted_numbers'][:3]]
            print(f"Predicted Numbers: {', '.join(top_numbers)}")

def test_emotion_detection_with_tokenization():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥"""
    
    expert = ExpertDreamInterpreter()
    
    emotion_tests = [
        {
            'input': "‡∏ù‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏á‡∏π‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ß‡∏°‡∏≤‡∏Å",
            'expected_emotions': ['fear'],
            'expected_symbols': ['‡∏á‡∏π']
        },
        {
            'input': "‡∏ù‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏ä‡πâ‡∏≤‡∏á‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°", 
            'expected_emotions': ['beautiful'],
            'expected_symbols': ['‡∏ä‡πâ‡∏≤‡∏á']
        },
        {
            'input': "‡∏ù‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏û‡∏£‡∏∞‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏á‡∏ö",
            'expected_emotions': ['peaceful'],
            'expected_symbols': ['‡∏û‡∏£‡∏∞']
        }
    ]
    
    print("\nüé≠ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥")
    print("=" * 60)
    
    for i, test in enumerate(emotion_tests, 1):
        print(f"\nüß™ Emotion Test {i}")
        print(f"Input: {test['input']}")
        
        result = expert.interpret_dream(test['input'])
        context = result.get('context_analysis', {})
        found_symbols = result.get('main_symbols', [])
        
        print(f"Context Found: {context}")
        print(f"Symbols Found: {found_symbols}")
        
        # Check emotion detection
        emotions_found = context.get('emotions', []) + context.get('size_modifiers', [])
        emotion_match = any(emo in emotions_found for emo in test['expected_emotions'])
        
        # Check symbol detection  
        symbol_match = any(sym in found_symbols for sym in test['expected_symbols'])
        
        print(f"Emotion Detection: {'‚úÖ' if emotion_match else '‚ùå'}")
        print(f"Symbol Detection:  {'‚úÖ' if symbol_match else '‚ùå'}")

def test_complex_sentence():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏à‡∏∏‡∏î"""
    
    expert = ExpertDreamInterpreter()
    
    complex_test = "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∑‡∏ô‡∏ù‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏á‡∏π‡πÉ‡∏´‡∏ç‡πà‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ß‡πÑ‡∏•‡πà‡∏Å‡∏±‡∏î‡∏â‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏û‡∏£‡∏∞‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏î‡∏≠‡∏Å‡πÑ‡∏°‡πâ‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°"
    
    print(f"\nüîÄ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô")
    print("=" * 60)
    print(f"Input: {complex_test}")
    
    # Show tokenization fix
    fixed = expert._fix_thai_tokenization(complex_test.lower())
    print(f"Fixed: {fixed}")
    
    # Full interpretation
    result = expert.interpret_dream(complex_test)
    
    print(f"\nSymbols Found: {result.get('main_symbols', [])}")
    print(f"Context: {result.get('context_analysis', {})}")
    print(f"Interpretation: {result.get('interpretation', '')[:150]}...")
    
    if result.get('predicted_numbers'):
        print(f"\nTop Numbers:")
        for pred in result['predicted_numbers'][:5]:
            print(f"  {pred['number']} - {pred['score']:.2f} - {pred['reason']}")

if __name__ == "__main__":
    try:
        test_tokenization_fixes()
        test_emotion_detection_with_tokenization()
        test_complex_sentence()
        
        print(f"\nüéâ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        print("‚úÖ ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")
        
    except Exception as e:
        print(f"\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
        import traceback
        traceback.print_exc()