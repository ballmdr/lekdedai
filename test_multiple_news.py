# -*- coding: utf-8 -*-
import os
import sys
import django
import requests
from bs4 import BeautifulSoup

# --- Setup Django ---
sys.path.append('/app')
# ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô Docker container ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ local database
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'lekdedai.settings')
django.setup()

from news.analyzer_switcher import AnalyzerSwitcher
from news.models import NewsCategory, NewsArticle
from lekdedai.utils import generate_unique_slug

def scrape_single_article(url):
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß"""
    
    print(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å: {url}")
    print()
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        title_elem = soup.find('h1')
        title = title_elem.get_text(strip=True) if title_elem else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠"
        
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        content_div = soup.find('div', class_='news-content')
        if not content_div:
            content_div = soup.find('div', {'id': 'news-content'})
        if not content_div:
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å article body
            content_div = soup.find('div', class_='article-body')
            
        if content_div:
            # ‡∏•‡∏ö elements ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            for unwanted in content_div.find_all(['script', 'style', 'ins', 'iframe', 'figure']):
                unwanted.decompose()
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            content = content_div.get_text(separator='\n', strip=True)
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            content = '\n'.join(line.strip() for line in content.split('\n') if line.strip())
        else:
            content = "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ"
        
        print("üì∞ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ:")
        print(f"‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {title}")
        print(f"‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {len(content)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
        print(f"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {content[:200]}...")
        print()
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI
        print("ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI...")
        analyzer = AnalyzerSwitcher(preferred_analyzer='groq')
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô
        is_relevant = analyzer.is_lottery_relevant(title, content)
        print(f"‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏ß‡∏¢: {is_relevant}")
        
        if is_relevant:
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°
            analysis = analyzer.analyze_news_for_lottery(title, content)
            print(f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:")
            print(f"- ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {analysis.get('category', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}")
            print(f"- ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {analysis.get('relevance_score', 0)}")
            print(f"- ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {analysis.get('numbers', [])}")
            print(f"- ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: {analysis.get('reasoning', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}")
            print()
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if analysis.get('success', False):
                print("üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
                
                # ‡∏´‡∏≤ category ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
                category_name = {
                    'accident': '‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏',
                    'politics': '‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á', 
                    'crime': '‡∏≠‡∏≤‡∏ä‡∏ç‡∏≤‡∏Å‡∏£‡∏£‡∏°',
                    'other': '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'
                }.get(analysis.get('category', 'other'), '‡∏≠‡∏∑‡πà‡∏ô‡πÜ')
                
                category, created = NewsCategory.objects.get_or_create(
                    name=category_name,
                    defaults={'description': f'‡∏Ç‡πà‡∏≤‡∏ß{category_name}‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ'}
                )
                
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                existing = NewsArticle.objects.filter(source_url=url).first()
                if existing:
                    print(f"‚ö†Ô∏è ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (ID: {existing.id})")
                    article = existing
                else:
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà
                    article = NewsArticle.objects.create(
                        title=title,
                        slug=generate_unique_slug(NewsArticle, title, None),
                        intro=content[:500],  # ‡πÉ‡∏ä‡πâ 500 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏≥
                        content=content,
                        source_url=url,
                        category=category,
                        lottery_relevance_score=analysis.get('relevance_score', 0),
                        lottery_category=analysis.get('category', 'other'),
                        extracted_numbers=','.join(analysis.get('numbers', [])),
                        confidence_score=80,  # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                        status='published'
                    )
                    print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (ID: {article.id})")
                
                # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó insight_entities ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏û‡∏ö
                if analysis.get('detailed_numbers'):
                    entities = []
                    for detail in analysis['detailed_numbers']:
                        entities.append({
                            'number': detail['number'],
                            'source': detail['source'],
                            'confidence': detail['confidence']
                        })
                    article.insight_entities = entities
                    article.save()
                    print(f"‚úÖ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó insight_entities: {len(entities)} ‡πÄ‡∏•‡∏Ç")
                    
                print(f"üåê ‡∏î‡∏π‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà: http://localhost:8000/news/{article.id}/")
                
                return {
                    'success': True,
                    'title': title,
                    'article_id': article.id,
                    'numbers': analysis.get('numbers', []),
                    'relevance_score': analysis.get('relevance_score', 0)
                }
        else:
            print("‚ùå ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏ß‡∏¢")
            return {'success': False, 'reason': 'Not relevant'}
        
        print()
        return {'success': False, 'reason': 'Analysis failed'}
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return {'success': False, 'error': str(e)}

def main():
    print("üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å Thairath")
    print("=" * 60)
    print()
    
    # URLs ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    test_urls = [
        "https://www.thairath.co.th/news/politic/2881640",
        "https://www.thairath.co.th/news/local/north/2881781", 
        "https://www.thairath.co.th/news/local/2882103"
    ]
    
    results = []
    
    for i, url in enumerate(test_urls, 1):
        print(f"üì∞ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà {i}/{len(test_urls)}")
        result = scrape_single_article(url)
        results.append({'url': url, 'result': result})
        print("=" * 60)
        print()
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    print("üìã ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö:")
    print("=" * 60)
    
    success_count = 0
    for i, item in enumerate(results, 1):
        url = item['url']
        result = item['result']
        
        if result['success']:
            success_count += 1
            print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà {i}: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (ID: {result['article_id']})")
            print(f"   ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {result['title'][:50]}...")
            print(f"   ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {result['numbers']}")
            print(f"   ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {result['relevance_score']}/100")
        else:
            print(f"‚ùå ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà {i}: ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({result.get('reason', 'Unknown error')})")
        
        print(f"   URL: {url}")
        print()
    
    print(f"üéâ ‡∏ú‡∏•‡∏£‡∏ß‡∏°: {success_count}/{len(test_urls)} ‡∏Ç‡πà‡∏≤‡∏ß‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

if __name__ == "__main__":
    main()