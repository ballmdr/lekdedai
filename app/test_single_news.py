# -*- coding: utf-8 -*-
import os
import sys
import django
import requests
from bs4 import BeautifulSoup

# --- Setup Django ---
sys.path.append('/app')
# ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô Docker container ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ local database
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'lekdedai.settings')
django.setup()

from news.analyzer_switcher import AnalyzerSwitcher
from news.models import NewsCategory, NewsArticle
from lekdedai.utils import generate_unique_slug

def scrape_single_article(url):
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß"""
    
    print(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å: {url}")
    print()
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        title_elem = soup.find('h1')
        title = title_elem.get_text(strip=True) if title_elem else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠"
        
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        content_div = soup.find('div', class_='news-content')
        if not content_div:
            content_div = soup.find('div', {'id': 'news-content'})
        if not content_div:
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å article body
            content_div = soup.find('div', class_='article-body')
            
        if content_div:
            # ‡∏•‡∏ö elements ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            for unwanted in content_div.find_all(['script', 'style', 'ins', 'iframe', 'figure']):
                unwanted.decompose()
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            content = content_div.get_text(separator='\n', strip=True)
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            content = '\n'.join(line.strip() for line in content.split('\n') if line.strip())
        else:
            content = "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ"
        
        print("üì∞ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ:")
        print(f"‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {title}")
        print(f"‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ({len(content)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£):")
        print("-" * 50)
        print(content[:800] + ("..." if len(content) > 800 else ""))
        print("-" * 50)
        print()
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI
        print("ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI...")
        analyzer = AnalyzerSwitcher(preferred_analyzer='groq')
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô
        is_relevant = analyzer.is_lottery_relevant(title, content)
        print(f"‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏ß‡∏¢: {is_relevant}")
        
        if is_relevant:
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°
            analysis = analyzer.analyze_news_for_lottery(title, content)
            print(f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:")
            print(f"- ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {analysis.get('category', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}")
            print(f"- ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {analysis.get('relevance_score', 0)}")
            print(f"- ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {analysis.get('numbers', [])}")
            print(f"- ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: {analysis.get('reasoning', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}")
            print()
            
            if analysis.get('detailed_numbers'):
                print("‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏û‡∏ö:")
                for detail in analysis['detailed_numbers']:
                    print(f"  - ‡πÄ‡∏•‡∏Ç {detail['number']}: {detail['source']} (‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à {detail['confidence']}%)")
        else:
            print("‚ùå ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏ß‡∏¢")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏ß‡∏¢
        article = None
        if is_relevant and analysis.get('success', False):
            print("üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
            
            # ‡∏´‡∏≤ category ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
            category = None
            try:
                category = NewsCategory.objects.get(name='‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏')
            except NewsCategory.DoesNotExist:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ category ‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
                category = NewsCategory.objects.create(
                    name='‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏',
                    description='‡∏Ç‡πà‡∏≤‡∏ß‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ'
                )
            
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            existing = NewsArticle.objects.filter(source_url=url).first()
            if existing:
                print(f"‚ö†Ô∏è ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (ID: {existing.id})")
                article = existing
            else:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà
                article = NewsArticle.objects.create(
                    title=title,
                    slug=generate_unique_slug(NewsArticle, title, None),
                    intro=content[:500],  # ‡πÉ‡∏ä‡πâ 500 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏≥
                    content=content,
                    source_url=url,
                    category=category,
                    lottery_relevance_score=analysis.get('relevance_score', 0),
                    lottery_category=analysis.get('category', 'accident'),
                    extracted_numbers=','.join(analysis.get('numbers', [])),
                    confidence_score=80,  # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                    status='published'
                )
                print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (ID: {article.id})")
            
            # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó insight_entities ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏û‡∏ö
            if analysis.get('detailed_numbers'):
                entities = []
                for detail in analysis['detailed_numbers']:
                    entities.append({
                        'number': detail['number'],
                        'source': detail['source'],
                        'confidence': detail['confidence']
                    })
                article.insight_entities = entities
                article.save()
                print(f"‚úÖ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó insight_entities: {len(entities)} ‡πÄ‡∏•‡∏Ç")
                
            print(f"üåê ‡∏î‡∏π‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà: http://localhost:8000/news/{article.id}/")
        
        print()
        return {
            'title': title,
            'content': content,
            'url': url,
            'is_relevant': is_relevant,
            'analysis': analysis if is_relevant else None,
            'article_id': article.id if article else None
        }
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return None

def main():
    print("üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏à‡∏≤‡∏Å Thairath")
    print()
    
    # URL ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    test_url = "https://www.thairath.co.th/news/local/northeast/2881357"
    
    result = scrape_single_article(test_url)
    
    if result:
        print("üéâ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    else:
        print("‚ùå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

if __name__ == "__main__":
    main()