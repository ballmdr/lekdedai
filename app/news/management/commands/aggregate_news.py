import requests
import json
import time
from datetime import datetime
from django.core.management.base import BaseCommand
from django.utils import timezone
from news.models import NewsArticle, NewsCategory
from news.gemini_lottery_analyzer import GeminiLotteryAnalyzer
from news.mock_gemini_analyzer import MockGeminiLotteryAnalyzer
from lekdedai.utils import generate_unique_slug
from bs4 import BeautifulSoup
import os

class Command(BaseCommand):
    help = 'Aggregate news from multiple sources into one article with lottery analysis'

    def add_arguments(self, parser):
        parser.add_argument(
            '--input',
            type=str,
            help='Input text with news list or JSON file path'
        )
        parser.add_argument(
            '--title',
            type=str,
            default='',
            help='Main title for aggregated news'
        )

    def handle(self, *args, **options):
        input_data = options.get('input', '')
        main_title = options.get('title', '')
        
        if not input_data:
            # ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì
            input_data = '''
Thai PBS News: "‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏î ‡∏®‡∏≤‡∏•‡∏Ø ‡∏™‡∏±‡πà‡∏á‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏© ‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å 1 ‡∏õ‡∏µ ‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô 14" ‚Äî https://www.thaipbs.or.th/news/content/356324
Thai PBS News: "‡πÄ‡∏õ‡∏¥‡∏î‡∏°‡∏ï‡∏¥‡πÄ‡∏≠‡∏Å‡∏â‡∏±‡∏ô‡∏ó‡πå 5-0 ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏© '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì' ‡∏Å‡∏•‡∏±‡∏ö‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å 1 ‡∏õ‡∏µ ‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô 14" ‚Äî https://www.thaipbs.or.th/news/content/356341
Thai PBS News: "‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏®‡∏≤‡∏•‡∏Ø ‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ï‡πá‡∏° ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏© '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì' ‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô 14" ‚Äî https://www.thaipbs.or.th/news/content/356333
BBC ‡πÑ‡∏ó‡∏¢: "‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏®‡∏≤‡∏•‡∏é‡∏µ‡∏Å‡∏≤ '‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô 14' ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‚Ä¶" ‚Äî https://www.bbc.com/thai/articles/c147l4ryx3yo
BBC ‡πÑ‡∏ó‡∏¢: "‡∏®‡∏≤‡∏•‡∏é‡∏µ‡∏Å‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏©‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì 1 ‡∏õ‡∏µ '‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô 14'" ‚Äî https://www.bbc.com/thai/articles/cm2z0j84gzvo
‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê: "‡∏®‡∏≤‡∏•‡∏é‡∏µ‡∏Å‡∏≤‡∏Ø ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏© '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì' ‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å 1 ‡∏õ‡∏µ ‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô 14" ‚Äî https://www.thairath.co.th/news/politic/2881616
‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê: "‡∏ä‡∏°‡∏™‡∏î ‡∏®‡∏≤‡∏•‡∏é‡∏µ‡∏Å‡∏≤‡∏Ø ‡∏ô‡∏±‡∏î‡∏ü‡∏±‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏© '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì' ‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô 14" ‚Äî https://www.thairath.co.th/news/politic/2881592
‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê: "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ï‡πá‡∏° ‡∏®‡∏≤‡∏•‡∏é‡∏µ‡∏Å‡∏≤‡∏Ø ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏© '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì' ‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å 1 ‡∏õ‡∏µ ‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô 14" ‚Äî https://www.thairath.co.th/news/politic/2881670
            '''
            main_title = '‡∏®‡∏≤‡∏•‡∏é‡∏µ‡∏Å‡∏≤‡∏™‡∏±‡πà‡∏á‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏©‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì ‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å 1 ‡∏õ‡∏µ ‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô 14 - ‡∏£‡∏ß‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏™‡∏≥‡∏ô‡∏±‡∏Å'
        
        self.stdout.write('=== News Aggregation & Lottery Analysis System ===')
        self.stdout.write(f'Main Title: {main_title}')
        
        # Parse ‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å input
        news_sources = self.parse_news_input(input_data)
        self.stdout.write(f'Found {len(news_sources)} news sources')
        
        # Scrape ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å URL
        articles_content = []
        for i, source in enumerate(news_sources):
            self.stdout.write(f'{i+1}. Scraping: {source["source"]} - {source["title"][:50]}...')
            content = self.scrape_article_content(source['url'])
            if content:
                articles_content.append({
                    'source': source['source'],
                    'title': source['title'],
                    'url': source['url'],
                    'content': content
                })
                self.stdout.write(f'   ‚úì Scraped {len(content)} characters')
            else:
                self.stdout.write(f'   ‚úó Failed to scrape')
            time.sleep(1)  # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á request
        
        if not articles_content:
            self.stdout.write(self.style.ERROR('No articles scraped successfully'))
            return
        
        # ‡∏£‡∏ß‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        aggregated_content = self.aggregate_articles(articles_content, main_title)
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏•‡∏Ç‡∏î‡πâ‡∏ß‡∏¢ Gemini AI
        self.stdout.write('Analyzing numbers with Gemini AI...')
        analyzer = self.get_analyzer()
        analysis = analyzer.analyze_news_for_lottery(main_title, aggregated_content['full_content'])
        
        if analysis.get('success') and analysis.get('is_relevant'):
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß
            saved_article = self.save_aggregated_news(
                main_title, 
                aggregated_content, 
                articles_content, 
                analysis
            )
            
            self.stdout.write(
                self.style.SUCCESS(
                    f'\nüéâ Successfully created aggregated news article!'
                )
            )
            self.stdout.write(f'üì∞ Title: {saved_article.title}')
            self.stdout.write(f'üî¢ Numbers: {analysis["numbers"]}')
            self.stdout.write(f'üìä Score: {analysis.get("relevance_score", 0)}')
            self.stdout.write(f'üìÇ Category: {analysis.get("category", "other")}')
            self.stdout.write(f'üåê Sources: {len(articles_content)} websites')
            self.stdout.write(f'üîó URL: http://localhost:8000{saved_article.get_absolute_url()}')
            
        else:
            self.stdout.write(
                self.style.ERROR(
                    f'Analysis failed or not lottery relevant: {analysis.get("error", "Unknown error")}'
                )
            )

    def parse_news_input(self, input_text):
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å input text"""
        news_sources = []
        lines = input_text.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
                
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: Source: "Title" ‚Äî URL
            if '‚Äî' in line and 'http' in line:
                parts = line.split('‚Äî')
                if len(parts) >= 2:
                    left_part = parts[0].strip()
                    url = parts[1].strip()
                    
                    # ‡πÅ‡∏¢‡∏Å source ‡πÅ‡∏•‡∏∞ title
                    if ':' in left_part:
                        source_title = left_part.split(':', 1)
                        source = source_title[0].strip()
                        title = source_title[1].strip().strip('"').strip("'")
                    else:
                        source = 'Unknown'
                        title = left_part.strip('"').strip("'")
                    
                    news_sources.append({
                        'source': source,
                        'title': title,
                        'url': url
                    })
        
        return news_sources

    def scrape_article_content(self, url):
        """Scrape ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å URL"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ‡∏•‡∏ö script, style, nav, footer
            for element in soup(['script', 'style', 'nav', 'footer', 'aside']):
                element.decompose()
            
            # ‡∏´‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å
            content_selectors = [
                'article', '.article-content', '.post-content', '.entry-content',
                '.content', 'main', '.main-content', '.story-content'
            ]
            
            content = ''
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = elements[0].get_text(separator=' ', strip=True)
                    break
            
            if not content:
                # fallback: ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á body
                body = soup.find('body')
                if body:
                    content = body.get_text(separator=' ', strip=True)
            
            return content[:2000]  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 2000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
            
        except Exception as e:
            self.stdout.write(f'Error scraping {url}: {e}')
            return None

    def aggregate_articles(self, articles_content, main_title):
        """‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á"""
        
        sources = list(set([article['source'] for article in articles_content]))
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà
        all_content = ' '.join([article['content'] for article in articles_content])
        all_titles = [article['title'] for article in articles_content]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á intro ‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏£‡∏¥‡∏á
        intro = f'‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á {", ".join(sources)} ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ'
        
        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        article_content = self.create_news_article(main_title, all_content, all_titles)
        
        return {
            'intro': intro,
            'full_content': article_content,
            'sources_count': len(articles_content),
            'sources': sources
        }
    
    def create_news_article(self, main_title, combined_content, titles):
        """‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß"""
        
        # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        key_info = self.extract_key_information(combined_content, titles)
        
        # ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏≤‡∏Å content
        specific_events = self.extract_specific_events(combined_content, titles)
        
        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏Ç‡πà‡∏≤‡∏ß
        article = f"{main_title}\n\n"
        
        # ‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å - ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏•‡∏±‡∏Å
        if key_info.get('main_event'):
            article += f"{key_info['main_event']}"
            if key_info.get('details'):
                article += f" {key_info['details']}"
            article += "\n\n"
        
        # ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        if specific_events:
            article += "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô:\n\n"
            for i, event in enumerate(specific_events[:5], 1):  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 5 ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÅ‡∏£‡∏Å
                # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
                clean_event = event.replace("‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏", "‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏")
                clean_event = clean_event.replace("  ", " ").strip()
                article += f"‚Ä¢ {clean_event}\n\n"
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        if key_info.get('background'):
            article += f"‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô {key_info['background']}\n\n"
        
        # ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        if key_info.get('impact'):
            article += f"‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ô‡∏µ‡πâ{key_info['impact']}\n\n"
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥)
        if key_info.get('numbers'):
            unique_numbers = list(dict.fromkeys(key_info['numbers']))  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
            article += f"‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {', '.join(unique_numbers[:10])}"  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 10 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
        
        return article
    
    def extract_specific_events(self, content, titles):
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏≤‡∏Å‡∏Ç‡πà‡∏≤‡∏ß"""
        import re
        
        events = []
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏
        if '‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏' in content or '‡∏à‡∏£‡∏≤‡∏à‡∏£' in content:
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å titles
            for title in titles:
                if '‡∏ß‡∏á‡πÅ‡∏´‡∏ß‡∏ô‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å' in title:
                    events.append("‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏‡∏ö‡∏ô‡∏ß‡∏á‡πÅ‡∏´‡∏ß‡∏ô‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å (‡∏ó‡∏•.9) ‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏Å‡∏°.62+700 ‡∏°‡∏∏‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ö‡∏≤‡∏á‡∏û‡∏•‡∏µ ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏à‡∏£‡∏≤‡∏à‡∏£‡∏ï‡∏¥‡∏î‡∏Ç‡∏±‡∏î")
                elif '‡∏£‡∏≤‡∏ä‡∏û‡∏§‡∏Å‡∏©‡πå' in title and '‡πÄ‡∏™‡∏µ‡∏¢‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï' in title:
                    events.append("‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏ö‡∏ô‡∏ñ‡∏ô‡∏ô‡∏£‡∏≤‡∏ä‡∏û‡∏§‡∏Å‡∏©‡πå ‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡∏≠‡∏¢‡∏ö‡∏≤‡∏á‡∏Å‡∏£‡πà‡∏≤‡∏á 12 ‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ ‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï")
                elif '‡∏õ‡∏£‡∏∞‡πÄ‡∏™‡∏£‡∏¥‡∏ê‡∏°‡∏ô‡∏π‡∏Å‡∏¥‡∏à' in title:
                    events.append("‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏‡∏ö‡∏ô‡∏ñ‡∏ô‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏™‡∏£‡∏¥‡∏ê‡∏°‡∏ô‡∏π‡∏Å‡∏¥‡∏à ‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡πÅ‡∏¢‡∏Å‡∏•‡∏≤‡∏î‡∏õ‡∏•‡∏≤‡πÄ‡∏Ñ‡πâ‡∏≤ ‡∏£‡∏ñ‡∏à‡∏±‡∏Å‡∏£‡∏¢‡∏≤‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡∏ä‡∏ô‡∏Å‡∏±‡∏ô ‡∏°‡∏µ‡∏ú‡∏π‡πâ‡∏ö‡∏≤‡∏î‡πÄ‡∏à‡πá‡∏ö")
                elif '‡∏â‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ä' in title:
                    events.append("‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏‡∏ö‡∏ô‡∏ó‡∏≤‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏â‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ä ‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏´‡∏•‡∏±‡∏á‡∏î‡πà‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏• 5 ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏£‡∏≤‡∏à‡∏£‡∏ï‡∏¥‡∏î‡∏Ç‡∏±‡∏î")
            
            # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ event ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å content
            if not events:
                if '‡∏Å‡∏°.62+700' in content:
                    events.append("‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏‡∏ö‡∏ô‡∏ß‡∏á‡πÅ‡∏´‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏≠‡∏Å‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø ‡∏™‡∏≤‡∏¢‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å ‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£‡∏ó‡∏µ‡πà 62+700")
                if '‡πÄ‡∏™‡∏µ‡∏¢‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï' in content:
                    events.append("‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏ö‡∏ô‡∏ñ‡∏ô‡∏ô‡∏™‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å")
                if '‡∏ï‡∏¥‡∏î‡∏Ç‡∏±‡∏î' in content:
                    events.append("‡∏Å‡∏≤‡∏£‡∏à‡∏£‡∏≤‡∏à‡∏£‡∏ï‡∏¥‡∏î‡∏Ç‡∏±‡∏î‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏à‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏ì‡∏ë‡∏•")
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á
        elif '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì' in content:
            # ‡∏´‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            events.append("‡∏®‡∏≤‡∏•‡∏é‡∏µ‡∏Å‡∏≤‡πÅ‡∏ú‡∏ô‡∏Å‡∏Ñ‡∏î‡∏µ‡∏≠‡∏≤‡∏ç‡∏≤‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏î‡∏≥‡∏£‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏©")
            
            if '‡∏°‡∏ï‡∏¥‡πÄ‡∏≠‡∏Å‡∏â‡∏±‡∏ô‡∏ó‡πå' in content:
                events.append("‡∏Ñ‡∏ì‡∏∞‡∏ú‡∏π‡πâ‡∏û‡∏¥‡∏û‡∏≤‡∏Å‡∏©‡∏≤‡∏°‡∏µ‡∏°‡∏ï‡∏¥‡πÄ‡∏≠‡∏Å‡∏â‡∏±‡∏ô‡∏ó‡πå 5-0 ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏©‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å")
            
            if '‡∏û‡∏±‡∏Å‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ï‡∏±‡∏ß' in content:
                events.append("‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö‡∏£‡∏ß‡∏° 120 ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏ï‡∏≥‡∏£‡∏ß‡∏à")
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ events ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÉ‡∏ä‡πâ title ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß
        if not events and titles:
            for title in titles[:3]:  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 3 ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏£‡∏Å
                if len(title.strip()) > 15:
                    events.append(title.strip())
        
        return events
    
    def extract_key_information(self, content, titles):
        """‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤"""
        import re
        
        key_info = {
            'numbers': [],
            'main_event': '',
            'details': '',
            'background': '',
            'impact': ''
        }
        
        # ‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏õ‡∏µ ‡∏û.‡∏®.)
        number_patterns = [
            r'‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å\s*(\d+)\s*‡∏õ‡∏µ',
            r'‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô\s*(\d+)',
            r'‡∏≠‡∏≤‡∏¢‡∏∏\s*(\d+)\s*‡∏õ‡∏µ',
            r'‡πÄ‡∏ß‡∏•‡∏≤\s*(\d{2})\.(\d{2})',
            r'‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô.*?(\d+)',
            r'‡∏°‡∏ï‡∏¥.*?(\d+)-(\d+)',
            r'(\d+)\s*‡∏ß‡∏±‡∏ô',
            r'(\d+)\s*‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó',
            r'(\d+)\s*‡πÅ‡∏™‡∏ô‡∏ö‡∏≤‡∏ó',
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏
            r'‡∏Å‡∏°\.(\d+)',
            r'‡∏Å‡∏°\.(\d+)\+(\d+)',
            r'‡∏ã‡∏≠‡∏¢\s*(\d+)',
            r'‡∏ó‡∏•\.(\d+)',
            r'(\d+)\s*‡∏Ç‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤',
            r'‡∏î‡πà‡∏≤‡∏ô.*?(\d+)',
            r'‡πÄ‡∏™‡∏µ‡∏¢‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï.*?(\d+)',
            r'(\d+)\s*‡∏Ñ‡∏±‡∏ô'
        ]
        
        for pattern in number_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    key_info['numbers'].extend([str(m) for m in match if m and m != ''])
                else:
                    # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
                    if match and match.isdigit():
                        num = int(match)
                        if 1 <= num <= 99 and num < 2500:  # ‡πÄ‡∏•‡∏Ç 1-99 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                            key_info['numbers'].append(match)
        
        # ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏´‡∏•‡∏±‡∏Å
        if '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì' in content and '‡∏®‡∏≤‡∏•' in content:
            key_info['main_event'] = '‡∏®‡∏≤‡∏•‡∏é‡∏µ‡∏Å‡∏≤‡πÅ‡∏ú‡∏ô‡∏Å‡∏Ñ‡∏î‡∏µ‡∏≠‡∏≤‡∏ç‡∏≤‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏î‡∏≥‡∏£‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ó‡∏©‡∏ô‡∏≤‡∏¢‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì ‡∏ä‡∏¥‡∏ô‡∏ß‡∏±‡∏ï‡∏£'
        elif '‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏' in content or '‡∏à‡∏£‡∏≤‡∏à‡∏£' in content:
            key_info['main_event'] = '‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏≤‡∏á‡∏à‡∏£‡∏≤‡∏à‡∏£‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏à‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£'
            
        # ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î - ‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á
        if '‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å' in content:
            jail_match = re.search(r'‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å\s*(\d+)\s*‡∏õ‡∏µ', content)
            if jail_match:
                key_info['details'] = f'‡πÉ‡∏´‡πâ‡∏à‡∏≥‡∏Ñ‡∏∏‡∏Å {jail_match.group(1)} ‡∏õ‡∏µ'
        
        if '‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô' in content:
            case_match = re.search(r'‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô\s*(\d+)', content)
            if case_match:
                if key_info['details']:
                    key_info['details'] += f' ‡πÉ‡∏ô‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô {case_match.group(1)}'
                else:
                    key_info['details'] = f'‡∏Ñ‡∏î‡∏µ‡∏ä‡∏±‡πâ‡∏ô {case_match.group(1)}'
        
        # ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î - ‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏
        if '‡∏ß‡∏á‡πÅ‡∏´‡∏ß‡∏ô' in content:
            key_info['details'] = '‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏ó‡∏≤‡∏á‡∏î‡πà‡∏ß‡∏ô‡∏ß‡∏á‡πÅ‡∏´‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏≠‡∏Å‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø ‡πÅ‡∏•‡∏∞‡∏ñ‡∏ô‡∏ô‡∏™‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å'
        elif '‡∏£‡∏≤‡∏ä‡∏û‡∏§‡∏Å‡∏©‡πå' in content:
            if not key_info['details']:
                key_info['details'] = '‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏ö‡∏ô‡∏ñ‡∏ô‡∏ô‡∏£‡∏≤‡∏ä‡∏û‡∏§‡∏Å‡∏©‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï'
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á
        if '‡∏°‡∏ï‡∏¥' in content and '‡πÄ‡∏≠‡∏Å‡∏â‡∏±‡∏ô‡∏ó‡πå' in content:
            key_info['background'] = '‡∏Ñ‡∏ì‡∏∞‡∏ú‡∏π‡πâ‡∏û‡∏¥‡∏û‡∏≤‡∏Å‡∏©‡∏≤‡∏°‡∏µ‡∏°‡∏ï‡∏¥‡πÄ‡∏≠‡∏Å‡∏â‡∏±‡∏ô‡∏ó‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏î‡∏µ‡∏ô‡∏µ‡πâ'
        elif 'FM91' in content or '‡∏™‡∏ß‡∏û' in content:
            key_info['background'] = '‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ß‡∏¥‡∏ó‡∏¢‡∏∏ FM91 ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏à‡∏£‡∏≤‡∏à‡∏£‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏ï‡∏•‡∏≠‡∏î‡∏ß‡∏±‡∏ô'
        
        # ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö
        if '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì' in content:
            key_info['impact'] = '‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡πÑ‡∏ó‡∏¢'
        elif '‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏' in content:
            key_info['impact'] = '‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏£‡∏≤‡∏à‡∏£‡πÉ‡∏ô‡πÄ‡∏Ç‡∏ï‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏ì‡∏ë‡∏•'
        
        return key_info

    def find_common_keywords(self, titles):
        """‡∏´‡∏≤‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÉ‡∏ô titles"""
        from collections import Counter
        
        # ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏à‡∏≤‡∏Å titles
        all_words = []
        for title in titles:
            # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥
            words = title.replace('"', '').replace("'", '').replace('‡∏Ø', '').split()
            all_words.extend([word for word in words if len(word) > 2])
        
        # ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà
        word_counts = Counter(all_words)
        
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ
        return [word for word, count in word_counts.most_common(5) if count >= 2]

    def get_analyzer(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á analyzer (Gemini ‡∏´‡∏£‡∏∑‡∏≠ Mock)"""
        if os.getenv('GEMINI_API_KEY'):
            try:
                return GeminiLotteryAnalyzer()
            except:
                self.stdout.write('Gemini failed, using Mock analyzer')
                return MockGeminiLotteryAnalyzer()
        else:
            return MockGeminiLotteryAnalyzer()

    def save_aggregated_news(self, main_title, aggregated_content, articles_content, analysis):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡∏ß‡∏°‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á category
        category, created = NewsCategory.objects.get_or_create(
            name='‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡∏ß‡∏°',
            defaults={'slug': 'aggregated-news', 'description': '‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏™‡∏≥‡∏ô‡∏±‡∏Å'}
        )
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á slug
        slug = generate_unique_slug(NewsArticle, main_title, None)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß
        article = NewsArticle.objects.create(
            title=main_title,
            slug=slug,
            category=category,
            intro=aggregated_content['intro'],
            content=aggregated_content['full_content'],
            extracted_numbers=','.join(analysis['numbers'][:15]),
            confidence_score=min(analysis.get('relevance_score', 50), 100),
            lottery_relevance_score=analysis.get('relevance_score', 50),
            lottery_category=analysis.get('category', 'other'),
            status='published',
            published_date=timezone.now(),
            source_url=articles_content[0]['url'] if articles_content else '',
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Gemini analysis
            insight_summary=analysis.get('reasoning', ''),
            insight_impact_score=analysis.get('relevance_score', 0) / 100,
            insight_entities=[
                {
                    'value': item['number'],
                    'entity_type': 'number',
                    'reasoning': item['source'],
                    'significance_score': item['confidence'] / 100
                } for item in analysis.get('detailed_numbers', [])
            ]
        )
        
        return article