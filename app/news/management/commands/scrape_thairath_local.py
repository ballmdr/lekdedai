# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
from django.core.management.base import BaseCommand
from django.utils import timezone
from django.db import transaction
from datetime import datetime
import time
import re

from news.models import NewsArticle, NewsCategory
from news.analyzer_switcher import AnalyzerSwitcher
from lekdedai.utils import generate_unique_slug

class Command(BaseCommand):
    help = '‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å Thairath Local News ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ Groq AI'

    def add_arguments(self, parser):
        parser.add_argument(
            '--limit',
            type=int,
            default=20,
            help='‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏∂‡∏á (default: 20)'
        )
        parser.add_argument(
            '--min-score',
            type=int,
            default=70,
            help='‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß (default: 70)'
        )
        parser.add_argument(
            '--delay',
            type=float,
            default=2.0,
            help='‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£ scrape (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ, default: 2.0)'
        )

    def handle(self, *args, **options):
        limit = options['limit']
        min_score = options['min_score']
        delay = options['delay']
        
        self.stdout.write(
            self.style.SUCCESS('üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß Thairath Local News')
        )
        self.stdout.write(f'üìä ‡∏à‡∏≥‡∏Å‡∏±‡∏î: {limit} ‡∏Ç‡πà‡∏≤‡∏ß, ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥: {min_score}, ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤: {delay}s')
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á analyzer
        try:
            analyzer = AnalyzerSwitcher(preferred_analyzer='groq')
            self.stdout.write(self.style.SUCCESS('‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Groq AI ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à'))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Groq AI: {e}'))
            return
        
        # ‡∏´‡∏≤/‡∏™‡∏£‡πâ‡∏≤‡∏á category
        category, created = NewsCategory.objects.get_or_create(
            name='‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô',
            defaults={
                'slug': 'local',
                'description': '‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡∏à‡∏≤‡∏Å Thairath'
            }
        )
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß
        scraped_count = 0
        saved_count = 0
        analyzed_count = 0
        
        try:
            # 1. ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å
            news_list = self.scrape_news_list(limit)
            
            if not news_list:
                self.stdout.write(self.style.WARNING('‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏î‡πÜ'))
                return
            
            self.stdout.write(f'üì∞ ‡∏û‡∏ö {len(news_list)} ‡∏Ç‡πà‡∏≤‡∏ß')
            
            # 2. ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß
            for i, news_item in enumerate(news_list, 1):
                self.stdout.write(f'\\nüîç [{i}/{len(news_list)}] {news_item["title"][:50]}...')
                
                try:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡πÉ‡∏ô database ‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏´‡∏°
                    if NewsArticle.objects.filter(source_url=news_item['url']).exists():
                        self.stdout.write('   ‚è≠Ô∏è ‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß - ‡∏Ç‡πâ‡∏≤‡∏°')
                        continue
                    
                    # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß
                    content_data = self.scrape_news_content(news_item['url'])
                    if not content_data:
                        self.stdout.write('   ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ')
                        continue
                    
                    scraped_count += 1
                    
                    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ Groq
                    self.stdout.write('   ü§ñ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ Groq AI...')
                    analysis = analyzer.analyze_news_for_lottery(
                        content_data['title'], 
                        content_data['content']
                    )
                    
                    analyzed_count += 1
                    
                    if analysis.get('success'):
                        relevance_score = analysis.get('relevance_score', 0)
                        numbers = analysis.get('numbers', [])
                        
                        self.stdout.write(f'   üìä ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {relevance_score}/100')
                        self.stdout.write(f'   üî¢ ‡πÄ‡∏•‡∏Ç: {", ".join(numbers[:10])}')
                        
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÑ‡∏´‡∏°
                        if relevance_score >= min_score and numbers:
                            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß
                            try:
                                article = self.save_article(content_data, analysis, category)
                                saved_count += 1
                                self.stdout.write(
                                    self.style.SUCCESS(f'   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(numbers)} ‡πÄ‡∏•‡∏Ç')
                                )
                                self.stdout.write(f'   üîó URL: {article.get_absolute_url()}')
                            except Exception as save_error:
                                self.stdout.write(
                                    self.style.ERROR(f'   ‚ùå Error saving: {save_error}')
                                )
                        else:
                            reason = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç" if not numbers else f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô {relevance_score} < {min_score}"
                            self.stdout.write(
                                self.style.WARNING(f'   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå: {reason}')
                            )
                    else:
                        error_msg = analysis.get('error', 'Unknown error')
                        self.stdout.write(
                            self.style.ERROR(f'   ‚ùå ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {error_msg}')
                        )
                    
                    # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤
                    if delay > 0:
                        time.sleep(delay)
                        
                except Exception as e:
                    self.stdout.write(
                        self.style.ERROR(f'   ‚ùå Error processing: {e}')
                    )
                    continue
        
        except KeyboardInterrupt:
            self.stdout.write('\\n‚èπÔ∏è ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ')
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'‚ùå Fatal error: {e}'))
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        self.stdout.write('\\n=== ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ===')
        self.stdout.write(f'üì∞ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß: {scraped_count} ‡∏Ç‡πà‡∏≤‡∏ß')
        self.stdout.write(f'ü§ñ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {analyzed_count} ‡∏Ç‡πà‡∏≤‡∏ß')  
        self.stdout.write(f'üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: {saved_count} ‡∏Ç‡πà‡∏≤‡∏ß')
        self.stdout.write(self.style.SUCCESS('üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!'))

    def scrape_news_list(self, limit):
        """‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å Thairath Local"""
        url = "https://www.thairath.co.th/news/local"
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'th,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ‡∏´‡∏≤ elements ‡∏Ç‡πà‡∏≤‡∏ß (‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° structure ‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á Thairath)
            news_items = []
            
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ pattern ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thairath
            selectors = [
                'article a[href*="/news/"]',  # Links ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ article
                '.news-item a',               # Class news-item
                '.content-card a',            # Class content-card  
                'a[href*="/news/local/"]',    # Local news links
                '.post-item a',               # Class post-item
                '[data-href*="/news/"]'       # Data attributes
            ]
            
            for selector in selectors:
                links = soup.select(selector)
                if links:
                    self.stdout.write(f'   ‚úÖ ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏î‡πâ‡∏ß‡∏¢ selector: {selector} ({len(links)} items)')
                    break
            else:
                # Fallback: ‡∏´‡∏≤‡∏ó‡∏∏‡∏Å link ‡∏ó‡∏µ‡πà‡∏°‡∏µ /news/ 
                links = soup.find_all('a', href=re.compile(r'/news/'))
                self.stdout.write(f'   üîç Fallback: ‡∏û‡∏ö {len(links)} news links')
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• links
            seen_urls = set()
            for link in links:
                href = link.get('href', '')
                if not href:
                    continue
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á absolute URL
                if href.startswith('/'):
                    full_url = 'https://www.thairath.co.th' + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß local
                if '/news/local/' not in full_url:
                    continue
                
                # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥
                if full_url in seen_urls:
                    continue
                seen_urls.add(full_url)
                
                # ‡∏´‡∏≤ title
                title = ''
                if link.get('title'):
                    title = link['title'].strip()
                elif link.text:
                    title = link.text.strip()
                elif link.find('img') and link.find('img').get('alt'):
                    title = link.find('img')['alt'].strip()
                
                if title and len(title) > 10:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ title ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
                    news_items.append({
                        'title': title,
                        'url': full_url,
                        'scraped_at': timezone.now()
                    })
                
                if len(news_items) >= limit:
                    break
            
            return news_items[:limit]
            
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'‚ùå Error scraping news list: {e}'))
            return []

    def scrape_news_content(self, url):
        """‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å URL"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ‡∏•‡∏ö elements ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            for element in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
                element.decompose()
            
            # ‡∏´‡∏≤ title
            title = ''
            title_selectors = ['h1', '.article-title', '.post-title', 'title']
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    if len(title) > 10:  # Title ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
                        break
            
            # ‡∏´‡∏≤ content
            content = ''
            content_selectors = [
                '.article-content', '.post-content', '.entry-content',
                '[class*="content"]', 'article', '.detail', 'main'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ‡∏•‡∏ö ads, social sharing
                    for unwanted in content_elem(['iframe', '.ads', '.social', '.share']):
                        unwanted.decompose()
                    
                    content = content_elem.get_text(separator=' ', strip=True)
                    if len(content) > 200:  # Content ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
                        break
            
            # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤ content ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏ä‡πâ body
            if not content:
                body = soup.find('body')
                if body:
                    content = body.get_text(separator=' ', strip=True)
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î content
            content = self.clean_content(content)
            
            if title and content and len(content) > 100:
                return {
                    'title': title.strip(),
                    'content': content[:2000],  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 2000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                    'url': url
                }
            
            return None
            
        except Exception as e:
            self.stdout.write(f'   ‚ùå Error scraping content: {e}')
            return None

    def clean_content(self, content):
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤"""
        if not content:
            return ""
        
        # ‡∏•‡∏ö‡∏Ç‡∏¢‡∏∞‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå
        junk_keywords = [
            'logo thairath', '‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å', '‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤', 'light', 'dark', 
            '‡∏ü‡∏±‡∏á‡∏Ç‡πà‡∏≤‡∏ß', '‡πÅ‡∏ä‡∏£‡πå‡∏Ç‡πà‡∏≤‡∏ß', 'facebook', 'twitter', 'line'
        ]
        
        content_lower = content.lower()
        if any(keyword in content_lower for keyword in junk_keywords):
            # ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á
            start_markers = [
                '‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤', '‡πÄ‡∏°‡∏∑‡πà‡∏≠', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡πÄ‡∏ß‡∏•‡∏≤', '‡∏ô‡∏≤‡∏¢', '‡∏î‡∏£.', 
                '‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏´‡∏ï‡∏∏', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', '‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß'
            ]
            
            for marker in start_markers:
                if marker in content:
                    start_pos = content.find(marker)
                    if start_pos > 50:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡∏¢‡∏∞‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£
                        content = content[start_pos:]
                        break
        
        # ‡∏•‡∏ö‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡πÄ‡∏Å‡∏¥‡∏ô, newlines ‡πÄ‡∏Å‡∏¥‡∏ô
        content = re.sub(r'\\s+', ' ', content)
        content = re.sub(r'\\n+', '\\n', content)
        
        return content.strip()

    def save_article(self, content_data, analysis, category):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏á database"""
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á slug ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
        slug = generate_unique_slug(NewsArticle, content_data['title'], None)
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° insight_entities ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Groq analysis
        insight_entities = []
        for item in analysis.get('detailed_numbers', []):
            insight_entities.append({
                'value': item.get('number', ''),
                'entity_type': 'number',
                'reasoning': item.get('source', ''),
                'significance_score': item.get('confidence', 0) / 100,
                'analyzer_type': 'groq'
            })
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß
        article = NewsArticle.objects.create(
            title=content_data['title'],
            slug=slug,
            category=category,
            intro=content_data['content'][:300] + '...' if len(content_data['content']) > 300 else content_data['content'],
            content=content_data['content'],
            extracted_numbers=','.join(analysis['numbers'][:15]),
            confidence_score=min(analysis.get('relevance_score', 50), 100),
            lottery_relevance_score=analysis.get('relevance_score', 50),
            lottery_category=analysis.get('category', 'other'),
            status='published',
            published_date=timezone.now(),
            source_url=content_data['url'],
            
            # Groq AI analysis data
            insight_summary=analysis.get('reasoning', ''),
            insight_impact_score=analysis.get('relevance_score', 0) / 100,
            insight_entities=insight_entities,
            insight_analyzed_at=timezone.now()
        )
        
        return article