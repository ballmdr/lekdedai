# -*- coding: utf-8 -*-
import os
import sys
import django
import requests
from bs4 import BeautifulSoup

# --- Setup Django ---
sys.path.append('/app')  
sys.path.append('C:/Users/ballm/Dropbox/lekdedai/app')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'lekdedai.settings')
django.setup()

from news.models import NewsArticle, NewsCategory
from lekdedai.utils import generate_unique_slug
from django.utils import timezone

# Import analyzers
from news.groq_lottery_analyzer import GroqLotteryAnalyzer
try:
    from news.gemini_lottery_analyzer import GeminiLotteryAnalyzer
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ö†Ô∏è Gemini analyzer not available")

def _clean_website_junk(content):
    """‡∏•‡∏ö‡∏Ç‡∏¢‡∏∞‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    import re
    
    if not content:
        return ""
    
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ç‡∏¢‡∏∞‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πâ‡∏ô ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á
    if any(keyword in content[:300].lower() for keyword in ['logo', 'thairath', '‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å', 'light', 'dark', '‡∏ü‡∏±‡∏á‡∏Ç‡πà‡∏≤‡∏ß']):
        # ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á
        start_markers = ['‡∏≠‡∏î‡∏µ‡∏ï‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ', '‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ', '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì ‡∏ä‡∏¥‡∏ô‡∏ß‡∏±‡∏ï‡∏£', '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì', '‡∏£‡∏≠‡∏á‡∏ô‡∏≤‡∏¢‡∏Å', '‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ']
        for marker in start_markers:
            if marker in content:
                start_pos = content.find(marker)
                if start_pos > 50:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡∏¢‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 50 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤
                    content = content[start_pos:]
                    break
    
    return content.strip()

def scrape_thairath_news(url):
    """Scrape ‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å Thairath URL"""
    print(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á scrape ‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ‡∏´‡∏≤ title
        title_elem = soup.find('h1') or soup.find('title')
        title = title_elem.get_text(strip=True) if title_elem else '‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å Thairath'
        
        # ‡∏´‡∏≤ content
        content_selectors = [
            '.entry-content', '.post-content', '.article-content',
            'article', '.content', 'main'
        ]
        
        content = ""
        for selector in content_selectors:
            elem = soup.select_one(selector)
            if elem:
                # ‡∏•‡∏ö script, style tags
                for tag in elem(['script', 'style', 'nav', 'footer', 'aside']):
                    tag.decompose()
                content = elem.get_text(separator=' ', strip=True)
                break
        
        if not content:
            # fallback ‡πÉ‡∏ä‡πâ body
            body = soup.find('body')
            if body:
                for tag in body(['script', 'style', 'nav', 'footer', 'aside']):
                    tag.decompose()
                content = body.get_text(separator=' ', strip=True)
        
        print(f"‚úÖ Scrape ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(content)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
        
        # Clean content ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å - ‡∏•‡∏ö‡∏Ç‡∏¢‡∏∞‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏≠‡∏≠‡∏Å
        cleaned_content = _clean_website_junk(content.strip())
        
        return {
            'title': title.strip(),
            'content': cleaned_content[:2000],  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 2000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
            'url': url
        }
        
    except Exception as e:
        print(f"‚ùå Error scraping: {e}")
        return None

def compare_analyzers(news_data):
    """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Groq ‡πÅ‡∏•‡∏∞ Gemini"""
    print("\n=== ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ===")
    print(f"üì∞ ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {news_data['title']}")
    print(f"üìù ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {len(news_data['content'])} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
    print()
    
    results = {}
    
    # 1. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Groq
    print("ü§ñ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Groq Llama 3.1 8B Instant...")
    try:
        groq_analyzer = GroqLotteryAnalyzer()
        groq_result = groq_analyzer.analyze_news_for_lottery(
            news_data['title'], 
            news_data['content']
        )
        results['groq'] = groq_result
        
        if groq_result['success']:
            print(f"‚úÖ Groq ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:")
            print(f"   üî¢ ‡πÄ‡∏•‡∏Ç: {', '.join(groq_result['numbers'])}")
            print(f"   üìä ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {groq_result.get('relevance_score', 0)}/100")
            print(f"   üìÇ ‡∏´‡∏°‡∏ß‡∏î: {groq_result.get('category', 'other')}")
            print(f"   üí° ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: {groq_result.get('reasoning', '')[:100]}...")
        else:
            print(f"‚ùå Groq ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {groq_result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"‚ùå Groq Error: {e}")
        results['groq'] = {'success': False, 'error': str(e)}
    
    print()
    
    # 2. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Gemini (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    if GEMINI_AVAILABLE:
        print("ü§ñ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Gemini...")
        try:
            gemini_analyzer = GeminiLotteryAnalyzer()
            gemini_result = gemini_analyzer.analyze_news_for_lottery(
                news_data['title'], 
                news_data['content']
            )
            results['gemini'] = gemini_result
            
            if gemini_result['success']:
                print(f"‚úÖ Gemini ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:")
                print(f"   üî¢ ‡πÄ‡∏•‡∏Ç: {', '.join(gemini_result['numbers'])}")
                print(f"   üìä ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {gemini_result.get('relevance_score', 0)}/100")
                print(f"   üìÇ ‡∏´‡∏°‡∏ß‡∏î: {gemini_result.get('category', 'other')}")
                print(f"   üí° ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: {gemini_result.get('reasoning', '')[:100]}...")
            else:
                print(f"‚ùå Gemini ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {gemini_result.get('error', 'Unknown error')}")
                
        except Exception as e:
            print(f"‚ùå Gemini Error: {e}")
            results['gemini'] = {'success': False, 'error': str(e)}
    else:
        print("‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° Gemini - ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ")
        
    return results

def save_with_analyzer(news_data, analysis, analyzer_type):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏∏ analyzer"""
    print(f"üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ {analyzer_type})...")
    
    # ‡∏´‡∏≤/‡∏™‡∏£‡πâ‡∏≤‡∏á category
    category_name = {
        'politics': '‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á',
        'accident': '‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏', 
        'crime': '‡∏≠‡∏≤‡∏ä‡∏ç‡∏≤‡∏Å‡∏£‡∏£‡∏°',
        'celebrity': '‡∏Ñ‡∏ô‡∏î‡∏±‡∏á',
        'other': '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'
    }.get(analysis.get('category', 'other'), '‡∏≠‡∏∑‡πà‡∏ô‡πÜ')
    
    category, created = NewsCategory.objects.get_or_create(
        name=category_name,
        defaults={
            'slug': analysis.get('category', 'other'),
            'description': f'‡∏Ç‡πà‡∏≤‡∏ß{category_name}‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI'
        }
    )
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á slug ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
    base_slug = generate_unique_slug(NewsArticle, news_data['title'], None)
    unique_slug = f"{base_slug}-{analyzer_type}"
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß
    article = NewsArticle.objects.create(
        title=f"{news_data['title']} (‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ {analyzer_type.upper()})",
        slug=unique_slug,
        category=category,
        intro=news_data['content'][:300] + '...' if len(news_data['content']) > 300 else news_data['content'],
        content=news_data['content'],
        extracted_numbers=','.join(analysis['numbers'][:15]),
        confidence_score=min(analysis.get('relevance_score', 50), 100),
        lottery_relevance_score=analysis.get('relevance_score', 50),
        lottery_category=analysis.get('category', 'other'),
        status='published',
        published_date=timezone.now(),
        source_url=news_data['url'],
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• AI analysis
        insight_summary=analysis.get('reasoning', ''),
        insight_impact_score=analysis.get('relevance_score', 0) / 100,
        insight_entities=[
            {
                'value': item['number'],
                'entity_type': 'number',
                'reasoning': item['source'],
                'significance_score': item['confidence'] / 100,
                'analyzer_type': analyzer_type  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• analyzer ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
            } for item in analysis.get('detailed_numbers', [])
        ]
    )
    
    print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {article.title}")
    print(f"üîó URL: http://localhost:8000{article.get_absolute_url()}")
    
    return article

def main():
    url = "https://www.thairath.co.th/news/politic/2881640"  # URL ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì
    
    print("=== ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Groq vs Gemini ===" )
    print()
    
    # 1. Scrape ‡∏Ç‡πà‡∏≤‡∏ß
    news_data = scrape_thairath_news(url)
    if not news_data:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ scrape ‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏î‡πâ")
        return
    
    # 2. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö analyzers
    results = compare_analyzers(news_data)
    
    # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ñ‡πâ‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à)
    print("\n=== ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ===")
    
    for analyzer_name, result in results.items():
        if result.get('success') and result.get('is_relevant') and result.get('numbers'):
            try:
                article = save_with_analyzer(news_data, result, analyzer_name)
                print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {analyzer_name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {article.extracted_numbers}")
            except Exception as e:
                print(f"‚ùå Error saving {analyzer_name}: {e}")
        else:
            print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° {analyzer_name} - ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏•‡∏Ç‡πÄ‡∏î‡πá‡∏î")
    
    # 4. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    print("\n=== ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö ===")
    
    if results.get('groq', {}).get('success') and results.get('gemini', {}).get('success'):
        groq_numbers = set(results['groq']['numbers'])
        gemini_numbers = set(results['gemini']['numbers'])
        
        print(f"ü§ñ Groq: {', '.join(results['groq']['numbers'])} (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {results['groq']['relevance_score']})")
        print(f"ü§ñ Gemini: {', '.join(results['gemini']['numbers'])} (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {results['gemini']['relevance_score']})")
        print(f"üîÑ ‡πÄ‡∏•‡∏Ç‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô: {', '.join(groq_numbers.intersection(gemini_numbers))}")
        print(f"üÜö ‡πÄ‡∏•‡∏Ç‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô: Groq ‡∏°‡∏µ {', '.join(groq_numbers - gemini_numbers)}, Gemini ‡∏°‡∏µ {', '.join(gemini_numbers - groq_numbers)}")
    
    print("\nüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö!")

if __name__ == "__main__":
    main()